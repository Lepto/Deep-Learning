{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The purpose of this assignment is to determine the success of the foundation, Alphabet Soup's funding of successful ventures.  The goal is to analyze a dataset, using machine learning and neural networks to predict whether the applicant will be successful with funding provided by Alphabet Soup.  The goal is to obtain a 75% or greater accuracy prediction.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "- Targets: The only target that was used was the data from \"IS_SUCCESSFUL\" column.  This column of data is the dataset that will help in gaining better insight and deeper understanding on Alphabet Soup's model to fund ventures.  The datatype in this column are integers and \"1\" denotes \"a successful venture\" and \"0' denotes an unsuccessful veture funded by Alphabet Soup.\n",
    "\n",
    "- Features: It was determined that all columns in the dataset (except for two) were considered for the overall analysis.  However, this will be revisited when optimizing our model at a later point.\n",
    "\n",
    "- Data was further processed by binning to improve the accuracy of the model and to reduce noise within the dataset.  Data that had low frequency of occurance were binned into an \"Other\" column.  Two features were binned: \"APPLICATION\" & \"CLASSIFICATION.\"\n",
    "\n",
    "- Dropped Features: In this assignment, columns containing the federal \"EIN\" or \"Employee Identification Number\" and \"Names\" were dropped from the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling, Training, and Evaluating the Model\n",
    "\n",
    "- In order to make this run as efficinet as possible, it was decided to use Tensorflow and Keras-tuner to let the computer decide the number of neurons.  A total of 43 columns or dimensions were determined from the dataframe (after binning).   \n",
    "\n",
    "- A function was created to train and run the model, establishing two layers and allowing Keras-tuner to decide the number of neurons in the first and second layer.\n",
    "\n",
    "- Then the code would compile the model optimized by 'adam' looking for accuracy.  The hyperparamters were tuned to search for the best values for the machine learning process.\n",
    "\n",
    "- The first run recorded a 73.3% accuracy after 60 trials; total run time was a 14 minutes and 39 seconds with two activation functions were performed (relu and tan).  This run did not produce an accuracy rate of 75% or greater, which meant that the model could be optimized more.\n",
    "\n",
    "- After some thought, it was decided to re-run the model with the \"NAME\" feature in the dataset.  Since the dataset actually indicated that the applicant may have applied for funding more than once, the frequency of the number of times an applicant applied for funding will also have an effect on the model.  The dataset was re-processed with the \"NAME\" feature and the result was an optimized model that returned a 79.2% accuracy rate after 60 trials; with a total run time of 29 minutes and 35 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Overall, the results from this model was greater than the 75% asked by the Alphabet Soup Foundation, using Tensorflow Keras-Tuner model.  Futher optimization could be realized by using classification models like Naive Bayes, which assumes that the predictors in the dataset are independent; in other words, the assumption that the features are unrelated to each other.  Using the Naive Bayes model, the output could categorized by how often or the probability how often or likely an event will happen, in this case how the features will affect the outcome of funding from the Alphabet Soup Foundation.\n",
    "\n",
    "- Bias and variances are crucial to make sure that there are no huge differences between the actual and predicted values.  Data exploration is needed to make sure that dataset isn't \"too basic\" or to simplistic that the model can't capture or determine the important features of the dataset.  Using groupby() of the objects and summarizing the dataset, may help to decrease any bias or variance in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
